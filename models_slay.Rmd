---
title: "models_slay"
author: "Richa"
date: "2024-04-26"
output: html_document
---

# Data Processing

```{r, message = F}
rm(list = ls())

library(caret)
library(tidyverse)
library(dplyr)
library(readr)
library(randomForest)
library(janitor)
library(xgboost)
```

```{r}
# read in train and test data
admissions_test <- readRDS("admissions_test.rdata")
admissions_train <- readRDS("admissions_train.rdata")
```

```{r}
admissions_test <- admissions_test %>% 
  mutate(readmission = as.factor(readmission)) %>% 
  clean_names()
admissions_train <- admissions_train %>%
  mutate(readmission = as.factor(readmission)) %>%
  clean_names()

# admissions_train_zeroes <- admissions_train %>% filter(readmission == 0)
# admissions_train_sample <- admissions_train_zeroes[sample(1:nrow(admissions_train_zeroes), 2063, replace = F),]
# 
# admissions_train <- bind_rows(admissions_train %>% filter(readmission == 1),
#                                        admissions_train_sample)
```

# subset data to columns we consider for predictors

```{r}
cols <- colnames(admissions_train)
admissions_train_subset <- admissions_train[, -c(1:4,9,57, 58)]
admissions_test_subset <- admissions_test[, -c(1:4,9,57, 58)]
```


# Random Forest Model

## RF: Cross-Validation

```{r}
set.seed(263)

# cross-validation
control <- trainControl(method='cv', 
                        number=10, 
                        search = 'grid')

# tuning 
# 
tunegrid <- expand.grid(.mtry = 10)
# = 1:(ncol(Hitters)-`)

rf_tune <- train(readmission ~ .,
                 data = admissions_train_subset,
                 method = 'rf',
                 metric = 'Accuracy',
                 tuneGrid  = tunegrid, 
                 trControl = control)

print(rf_tune)

## best mtry
mtry_min <- which.min(rf_tune$results$Accuracy)
mtry_min
```

## RF: Final Model

```{r}
class_weights <- ifelse(admissions_train_subset$readmission == 1, 
                        sum(admissions_train_subset$readmission == 0)/sum(admissions_train_subset$readmission == 1), 1) %>% unique()
rf_final <- randomForest(readmission ~ ., data = admissions_train_subset,
                         mtry = 30,
                         importance = TRUE,
                         ntree = 200,
                         cutoff = c(0.65, 0.35))

# The default cutoff is c(0.5, 0.5); Increasing cutoff to c(0.65, 0.35) shifts a lot of predictions from 0 to 1 (as expected)

# classwt 
# different mtry
importance(rf_final)
varImpPlot(rf_final)
```

### RF: Predictions

```{r}
# on training data
train_predict_rf <- predict(rf_final, admissions_train_subset)
# on testing data
test_predict_rf <- predict(rf_final, admissions_test_subset)
```

### RF: Confusion Matrices

```{r}
rf_final$confusion
```

```{r}
table(test_predict_rf)
table(admissions_test_subset$readmission)

admissions_test_subset %>% 
  mutate(pred = test_predict_rf,
         accuracy = ifelse(pred == readmission, 1, 0)) %>% 
  group_by() %>% summarize(mean(accuracy))
```

### RF: Out-of-Bag

```{r}
plot(rf_final)
```


Conclusion on RF: It sucks. We have a 74.23% accuracy rate on our test data. 

# XGBoost

```{r}
admissions_train_X = data.matrix(admissions_train_subset[,1:51]) # making sure readmission is not fed as a predictor
admissions_train_Y = as.numeric(admissions_train %>% dplyr::pull(readmission)) - 1
admissions_test_X = data.matrix(admissions_test_subset[,1:51])
admissions_test_Y = as.numeric(admissions_test %>% dplyr::pull(readmission)) - 1

# ix_train = sample(1:length(ames_Y), 0.75*length(ames_Y))
# ix_test = setdiff(1:length(ames_Y), ix_train)

parameters = expand.grid(
  eta = seq(0.01, 0.5, length.out=10),
  #eta = seq(0.1, 1, length.out=10), # learning rate
  max.depth = c(2, 4, 6, 8, 10), # maximum depth of tree
  subsample = c(0.25, 0.5, 1), # subsampling ratio
  nrounds = c(2, 4, 6, 8, 10) # maximum number of boosting iterations
)

parameters$train_error = rep(0, nrow(parameters))
parameters$test_error = rep(0, nrow(parameters))
for (i in 1:nrow(parameters)) {
  fit_xgboost = xgboost(data = admissions_train_X, label = admissions_train_Y, 
                        eta = parameters$eta[i],
                        max.depth = parameters$max.depth[i],
                        nrounds = parameters$nrounds[i], 
                        subsample = parameters$subsample[i],
                        objective = "reg:logistic", 
                        verbose = F)
  
  pred_xgboost_probs = predict(fit_xgboost, admissions_test_X)
  pred_xgboost <- ifelse(pred_xgboost_probs > 0.5, 1, 0)
  
  parameters$train_error[i] = tail(fit_xgboost$evaluation_log$train_rmse, 1)
  parameters$test_error[i] = mean((pred_xgboost != admissions_test_Y))
}

#picking test error?
parameters %>% 
  group_by(nrounds) %>% 
  filter(train_error == min(train_error))
```

```{r}
table(predicted = pred_xgboost, actual = admissions_test_Y)
```


```{r}
##Code below is purely for experimenting
test_fit <- xgboost(data = admissions_train_X, 
                    label = admissions_train_Y, 
                    eta = parameters$eta[1],
                    max.depth = parameters$max.depth[1],
                    nrounds = parameters$nrounds[1], 
                    subsample = parameters$subsample[1],
                    #reg:logistic "logistic regression" vs. binary:logistic "log. regression for binary classification. output prob"
                    objective = "binary:logistic", 
                    verbose = F)

test_pred_probs <- predict(test_fit, admissions_test_X)
test_pred <- ifelse(test_pred_probs > 0.5, 1, 0)
mean(admissions_test_Y != test_pred)
```



