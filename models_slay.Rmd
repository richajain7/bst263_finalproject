---
title: "models_slay"
author: "Richa"
date: "2024-04-26"
output: html_document
---

# Data Processing

```{r, message = F}
rm(list = ls())

library(caret)
library(tidyverse)
library(dplyr)
library(readr)
library(randomForest)
library(janitor)
library(xgboost)
library(glmnet)
```

```{r}
# read in train and test data
admissions_test <- readRDS("admissions_test.rdata")
admissions_train <- readRDS("admissions_train.rdata")
```

# subset data to columns we consider for predictors

```{r}
cols <- colnames(admissions_train)
admissions_train_subset <- admissions_train[, -c(1:4,9,57, 58)]
admissions_test_subset <- admissions_test[, -c(1:4,9,57, 58)]
```

# Random Forest Model

## RF: Cross-Validation

```{r}
set.seed(263)

# cross-validation
control <- trainControl(method='cv', 
                        number=10, 
                        search = 'grid')

# tuning 
# 
tunegrid <- expand.grid(.mtry = 1:(ncol(admissions_train_subset)-1))
# = 1:(ncol(Hitters)-`)

rf_tune <- train(readmission ~ .,
                 data = admissions_train_subset,
                 method = 'rf',
                 metric = 'Accuracy',
                 tuneGrid  = tunegrid, 
                 trControl = control)

print(rf_tune)

## best mtry
mtry_max <- which.max(rf_tune$results$Accuracy)
mtry_max

# result of max mtry = 11
# result of min mtry = 50
```

## RF: Final Model

```{r}
class_weights <- ifelse(admissions_train_subset$readmission == 1, 
                        sum(admissions_train_subset$readmission == 0)/sum(admissions_train_subset$readmission == 1), 1) %>% unique()
rf_final <- randomForest(readmission ~ ., data = admissions_train_subset,
                         mtry = 11,
                         importance = TRUE,
                         ntree = 200,
                         cutoff = c(0.65, 0.35))

# The default cutoff is c(0.5, 0.5); Increasing cutoff to c(0.65, 0.35) shifts a lot of predictions from 0 to 1 (as expected)

# classwt 
# different mtry
importance(rf_final)
varImpPlot(rf_final)
```

### RF: Predictions

```{r}
# on training data
train_predict_rf <- predict(rf_final, admissions_train_subset)
# on testing data
test_predict_rf <- predict(rf_final, admissions_test_subset)
```

### RF: Confusion Matrices

```{r}
rf_final$confusion
```

```{r}
table(test_predict_rf)
table(admissions_test_subset$readmission)

admissions_test_subset %>% 
  mutate(pred = test_predict_rf,
         accuracy = ifelse(pred == readmission, 1, 0)) %>% 
  group_by() %>% summarize(mean(accuracy))
```

### RF: Out-of-Bag

```{r}
plot(rf_final)
```


Conclusion on RF: It sucks. We have a 74.23% accuracy rate on our test data. 

# XGBoost

```{r}
admissions_train_X = data.matrix(admissions_train_subset[,1:51]) # making sure readmission is not fed as a predictor
admissions_train_Y = as.numeric(admissions_train %>% dplyr::pull(readmission)) - 1
admissions_test_X = data.matrix(admissions_test_subset[,1:51])
admissions_test_Y = as.numeric(admissions_test %>% dplyr::pull(readmission)) - 1

# ix_train = sample(1:length(ames_Y), 0.75*length(ames_Y))
# ix_test = setdiff(1:length(ames_Y), ix_train)

parameters = expand.grid(
  #eta = seq(0.01, 0.5, length.out=10),
  eta = seq(0.1, 1, length.out=10), # learning rate
  max.depth = c(2, 4, 6, 8, 10), # maximum depth of tree
  subsample = c(0.25, 0.5, 1), # subsampling ratio
  nrounds = c(2, 4, 6, 8, 10) # maximum number of boosting iterations
)

parameters$train_error = rep(0, nrow(parameters))
parameters$test_error = rep(0, nrow(parameters))
for (i in 1:nrow(parameters)) {
  fit_xgboost = xgboost(data = admissions_train_X, label = admissions_train_Y, 
                        eta = parameters$eta[i],
                        max.depth = parameters$max.depth[i],
                        nrounds = parameters$nrounds[i], 
                        subsample = parameters$subsample[i],
                        objective = "reg:logistic", 
                        verbose = F)
  
  pred_xgboost_probs = predict(fit_xgboost, admissions_test_X)
  pred_xgboost <- ifelse(pred_xgboost_probs > 0.5, 1, 0)
  
  parameters$train_error[i] = tail(fit_xgboost$evaluation_log$train_rmse, 1)
  parameters$test_error[i] = mean((pred_xgboost != admissions_test_Y))
}

#picking test error?
parameters %>% 
  group_by(nrounds) %>% 
  filter(train_error == min(train_error))
```

```{r}
table(predicted = pred_xgboost, actual = admissions_test_Y)
```


```{r}
##Code below is purely for experimenting
test_fit <- xgboost(data = admissions_train_X, 
                    label = admissions_train_Y, 
                    eta = parameters$eta[1],
                    max.depth = parameters$max.depth[1],
                    nrounds = parameters$nrounds[1], 
                    subsample = parameters$subsample[1],
                    #reg:logistic "logistic regression" vs. binary:logistic "log. regression for binary classification. output prob"
                    objective = "binary:logistic", 
                    verbose = F)

test_pred_probs <- predict(test_fit, admissions_test_X)
test_pred <- ifelse(test_pred_probs > 0.5, 1, 0)
mean(admissions_test_Y != test_pred)
```


# Penalized Regression - Ridge

- Need to do CV to choose lambda

```{r}
## create an x matrix and y vector from the Hitters data.frame
## this is needed for input to glmnet
x <- model.matrix(readmission~.,admissions_train_subset)[,-1]
y <- admissions_train_subset$readmission
x_test <- model.matrix(readmission~.,admissions_test_subset)[,-1]
y_test <- admissions_test_subset$readmission
if(is.factor(y)) {
  y <- as.numeric(as.character(y))
}
if(is.factor(y_test)) {
  y_test <- as.numeric(as.character(y_test))
}

## define a grid of lambda values to try ##
## note that the lambdas are in descending order, i.e., largest values are first ##
#grid <- 10^seq(10,-2,length=100)
grid <- 10^seq(4,-2,length=100)
## fit ridge for each lambda values using glmnet ##
ridge.mod <- glmnet(x,y,alpha=0,lambda=grid)
## structure of returned model coefficients ##
dim(coef(ridge.mod))
plot(ridge.mod, xvar="lambda",label=T)
set.seed(263)
## use cross-validation to choose lambda ridge
cv.out=cv.glmnet(x,y,alpha=0)
plot(cv.out)

bestlam=cv.out$lambda.min
bestlam
```

```{r}
# Train/test split
set.seed(263)
## fit on train set
ridge.mod=glmnet(x,y,alpha=0,lambda=bestlam, thresh=1e-12)
## assess MSE on test set when lambda=4
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test)
mean((ridge.pred-y_test)^2)
#predict(ridge.mod,s=50,type="coefficients")
```


# Penalized Regression - Lasso

- Need to do CV to choose a lambda

```{r}
cv.out.lasso=cv.glmnet(x,y,alpha=1)
plot(cv.out.lasso)

bestlam.lasso=cv.out.lasso$lambda.min
bestlam.lasso

lasso.mod=glmnet(x,y,alpha=1,lambda=bestlam.lasso)
## path plot
plot(lasso.mod, xvar="lambda", label=T, xlim=c(-5,6))

lasso.pred = predict(lasso.mod, s=bestlam.lasso, newx=x_test)
mean(abs(lasso.pred-y_test))
# same error as ridge??

## look at the lasso coefs when using bestlam
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef

## nonzero coefs
lasso.coef[lasso.coef!=0]  

## best subset coefs using same number of variables 
#coef(regfit.full, sum(lasso.coef != 0)-1)

#plot(lm.pvalues,lasso.coef!=0,pch=20)
```







