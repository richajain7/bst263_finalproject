---
title: "models_slay"
author: "Richa"
date: "2024-04-26"
output: html_document
---

# Data Processing

```{r, message = F}
rm(list = ls())

library(caret)
library(tidyverse)
library(dplyr)
library(readr)
library(randomForest)
library(janitor)
library(xgboost)
library(glmnet)
library(table1)
library(kableExtra)
```

```{r}
admissions_test <- readRDS("admissions_test.rdata")
admissions_train <- readRDS("admissions_train.rdata")
```

#### creating separate vector for test truth

```{r}
test_true <- admissions_test$readmission
```

#### converting readmission to factor & cleaning names

```{r}
admissions_test <- admissions_test %>% 
  mutate(readmission = as.factor(readmission)) %>% 
  clean_names()
admissions_train <- admissions_train %>%
  mutate(readmission = as.factor(readmission)) %>%
  clean_names()
```

#### subset data to columns we consider for predictors

```{r}
cols <- colnames(admissions_train)
admissions_train_subset <- admissions_train[, -c(1:4,9,57)]
admissions_test_subset <- admissions_test[, -c(1:4,9,57)]
```

# Random Forest Model

## RF: Cross-Validation

```{r, eval = FALSE}
set.seed(263)

# cross-validation
control <- trainControl(method='cv', 
                        number=10, 
                        search = 'grid')

# tuning 
tunegrid <- expand.grid(.mtry = 1:(ncol(admissions_train_subset)-1))

rf_tune <- train(readmission ~ .,
                 data = admissions_train_subset,
                 method = 'rf',
                 metric = 'Accuracy',
                 tuneGrid  = tunegrid, 
                 trControl = control)

print(rf_tune)

## best mtry
mtry_max <- which.max(rf_tune$results$Accuracy)
mtry_max # result = 11
```

## RF: Final Model

```{r}
set.seed(263)
class_weights <- ifelse(admissions_train_subset$readmission == 1, 
                        sum(admissions_train_subset$readmission == 0)/sum(admissions_train_subset$readmission == 1), 1) %>% unique()
rf_final <- randomForest(readmission ~ ., data = admissions_train_subset,
                         mtry = 11,
                         importance = TRUE,
                         ntree = 200,
                         cutoff = c(0.5, 0.5))

#importance(rf_final)
#varImpPlot(rf_final)
```

## RF: Predictions

```{r}
set.seed(263)
# on training data
train_predict_rf <- predict(rf_final, admissions_train_subset, type = "prob")
# on testing data
test_predict_rf <- predict(rf_final, admissions_test_subset, type = "prob")
# save test probabilities into a separate variable
predictions_random_forest <- test_predict_rf[,2]
```

## RF: Confusion Matrices

```{r}
set.seed(263)
test_predict_rf_binary <- predict(rf_final, admissions_test_subset, type = "response")
rf_final$confusion
```

```{r}
table(test_predict_rf)
table(admissions_test_subset$readmission)

admissions_test_subset %>% 
  mutate(pred = test_predict_rf_binary,
         accuracy = ifelse(pred == readmission, 1, 0)) %>% 
  group_by() %>% summarize(mean(accuracy))
```

## RF: Out-of-Bag

```{r}
plot(rf_final)
```

# XGBoost

```{r}
admissions_train_X = data.matrix(admissions_train_subset[,1:51]) 
# making sure readmission is not fed as a predictor
admissions_train_Y = as.numeric(admissions_train %>% dplyr::pull(readmission)) - 1
admissions_test_X = data.matrix(admissions_test_subset[,1:51])
admissions_test_Y = as.numeric(admissions_test %>% dplyr::pull(readmission)) - 1

parameters = expand.grid(
  eta = seq(0.1, 1, length.out=10), # learning rate
  max.depth = c(2, 4, 6, 8, 10), # maximum depth of tree
  subsample = c(0.25, 0.5, 1), # subsampling ratio
  nrounds = c(2, 4, 6, 8, 10) # maximum number of boosting iterations
)

parameters$train_error = rep(0, nrow(parameters))
parameters$test_error = rep(0, nrow(parameters))
for (i in 1:nrow(parameters)) {
  fit_xgboost = xgboost(data = admissions_train_X, label = admissions_train_Y, 
                        eta = parameters$eta[i],
                        max.depth = parameters$max.depth[i],
                        nrounds = parameters$nrounds[i], 
                        subsample = parameters$subsample[i],
                        objective = "reg:logistic", 
                        verbose = F)
  
  pred_train_xgboost_probs = predict(fit_xgboost, admissions_train_X)
  pred_train_xgboost <- ifelse(pred_train_xgboost_probs > 0.5, 1, 0)
  
  pred_xgboost_probs = predict(fit_xgboost, admissions_test_X)
  pred_xgboost <- ifelse(pred_xgboost_probs > 0.5, 1, 0)
  
  parameters$train_error[i] = mean(abs(pred_train_xgboost != admissions_train_Y))
  parameters$test_error[i] = mean(abs(pred_xgboost != admissions_test_Y))
}

parameters %>% 
  group_by(nrounds) %>% 
  filter(train_error == min(train_error))
```

```{r}
table(predicted = pred_xgboost, actual = admissions_test_Y)
```
```{r}
#FINAL MODEL XGBOOST
fit_xgboost_final = xgboost(data = admissions_test_X, label = admissions_test_Y, 
                        eta = 0.8,
                        max.depth = 10,
                        nrounds = 10, 
                        subsample = 1,
                        objective = "reg:logistic", 
                        verbose = F)

# saving probabilities of 1s
predictions_xgboost <- predict(fit_xgboost_final, admissions_test_X)
```


# Penalized Regression - Ridge

```{r}
x <- model.matrix(readmission~.,admissions_train_subset)[,-1]
y <- admissions_train_subset$readmission
x_test <- model.matrix(readmission~.,admissions_test_subset)[,-1]
y_test <- admissions_test_subset$readmission
if(is.factor(y)) {
  y <- as.numeric(as.character(y))
}
if(is.factor(y_test)) {
  y_test <- as.numeric(as.character(y_test))
}

grid <- 10^seq(4,-2,length=100)
ridge.mod <- glmnet(x,y,family="binomial",alpha=0,lambda=grid)
dim(coef(ridge.mod))
plot(ridge.mod, xvar="lambda",label=T)
set.seed(263)

#CV
cv.out=cv.glmnet(x,y,alpha=0)
plot(cv.out)

bestlam=cv.out$lambda.min
bestlam
```

```{r}
# Train/test split
set.seed(263)
## fit on train set
ridge.mod=glmnet(x,y,family="binomial",alpha=0,lambda=bestlam, thresh=1e-12)
# predict
ridge.pred=predict(ridge.mod,s=bestlam,newx=x_test,type="response")
# check MAE
threshold <- 0.3 #lower prob for 1s
binary_predictions_ridge <- ifelse(ridge.pred >= threshold, 1, 0)
sum(binary_predictions_ridge)
mean(abs(binary_predictions_ridge-y_test))
# save probabilities
ridge_probs <- ridge.pred
```

```{r}
## look at the ridge coefs when using bestlam
out_ridge=glmnet(x,y,family = "binomial", alpha=0,lambda=grid)
ridge_coef=predict(out_ridge,type="coefficients",s=bestlam)
#ridge_coef
```

# Penalized Regression - Lasso

```{r}
set.seed(263)
cv.out.lasso=cv.glmnet(x,y,family = "binomial", alpha=1)
plot(cv.out.lasso)

bestlam.lasso=cv.out.lasso$lambda.min
bestlam.lasso

lasso.mod=glmnet(x,y,family = "binomial", alpha=1,lambda=bestlam.lasso)

lasso.pred = predict(lasso.mod, s=bestlam.lasso, newx=x_test, type = "response")
threshold <- 0.3 #lower prob for 1s
binary_predictions_lasso <- ifelse(lasso.pred >= threshold, 1, 0)
sum(binary_predictions_lasso)
mean(abs(binary_predictions_lasso-y_test))
lasso_probs <- lasso.pred

## look at the lasso coefs when using bestlam
out=glmnet(x,y,family = "binomial", alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam.lasso)
lasso.coef

## nonzero coefs
lasso.coef[lasso.coef!=0]  
```


# Histograms

```{r}
par(mfrow = c(2,2))
# rf
hist(predictions_random_forest, main = "Random Forest Predicted Probabilities", xlab = "")

#xgboost
hist(pred_xgboost_probs, main = "XGBoost Predicted Probabilities", xlab = "")

#ridge 
hist(ridge_probs, main = "Ridge Predicted Probabilities", xlab = "")

#lasso
hist(lasso_probs, main = "Lasso Predicted Probabilities", xlab = "")
```

# Ensemble Method

```{r}
# Average out all probabilities across models
ensembled.probs <- data.frame(
  rf = predictions_random_forest,
  xgboost = pred_xgboost_probs,
  lasso = lasso_probs
)
colnames(ensembled.probs) <- c("rf", "xgboost", "lasso")
ensembled.probs <- ensembled.probs %>% 
  rowwise() %>% 
  mutate(mean_prob = mean(c(rf, xgboost, lasso)))
hist(ensembled.probs$mean_prob, main = "Ensembled Predicted Probabilities", xlab = "")

# Test different thresholds
#threshold <- 0.5
#ensembled.pred <- ifelse(ensembled.probs >= threshold, 1, 0)
```

## Ensemble: AUC Curve for different thresholds

```{r}
# test all thresholds 
thresholds <- seq(0, 1, 0.01)
auc <- c()
titles <- c("Random Forest", "XGBoost","Lasso","Ensemble")
plots <- list()
# generating auc curve for each different method AND overall
# 1 - RF, 2 - XGBoost, 3 - Lasso, 4 - Mean
for(col in 1:4){
  sensitivity_vals <- c()
  specificity_vals <- c()
  
  for(t in thresholds){
    
    ensembled.pred <- ifelse(ensembled.probs[,col] >= t, 1, 0)
    ensembled.pred <- as.factor(ensembled.pred)
    test_true <- as.factor(test_true)
    sens <- sensitivity(ensembled.pred, test_true)
    spec <- specificity(ensembled.pred, test_true)
    sensitivity_vals <- c(sensitivity_vals, sens)
    specificity_vals <- c(specificity_vals, spec)
    
  }
  par(mfrow = c(2, 2))
  n <- 100
  colors <- colorRampPalette(c("red", "yellow", 
                               "green", "blue"))(n)
  
  # Draw the gradient legend
  z <- matrix(1:n, nrow = 1)
  legend_x <- 1 - min(specificity_vals)  
  legend_y <- min(sensitivity_vals)  
  legend_width <- diff(range(specificity_vals)) / 4 
  legend_height <- diff(range(sensitivity_vals)) / 8  
  layout(1)
  layout(matrix(1:2, ncol = 2), widths = c(5, 1), heights = c(1,1))  
  plot(1-specificity_vals, sensitivity_vals, main = titles[col], 
       type = "l", xlab = "False Positive Rate",ylab = "True Positive Rate")
  axis(4, at = seq(0, 1, 0.1), labels = seq(0, 1, 0.1))  
  abline(0,1)
  
  for (i in 1:(length(sensitivity_vals)-1)) {
    
  segments(1-specificity_vals[i], sensitivity_vals[i], 
           1-specificity_vals[i+1], sensitivity_vals[i+1], 
           col = colors[i * n / length(sensitivity_vals)], lwd = 2)
    
  }
  
  par(mar = c(5, 4, 4, 1.5))  
  image(z = z, col = colors, xaxt = "n", yaxt = "n", bty = "n", xlab = "", ylab = "", main="Cutoff")
  axis(1, at = seq(0, 1, length.out = n), labels = seq(0, 1, length.out = n), 
       las = 1, cex.axis = 0.7) 
  box() 
  plots[[col]] <- recordPlot()
  # Return to the previous plotting parameters
  layout(1)

  # Calculate AUC using the trapezoidal rule
  auc <- c(auc, sum(diff(1-specificity_vals) * (sensitivity_vals[-1] + 
                                           sensitivity_vals[-length(sensitivity_vals)]) / 2))
}
```

## AUC values

```{r}
# RF, XGBoost, Lasso, Mean
auc #higher the better
```


- great accuracy if everyone was a zero
- breakdown to false positives and false negatives
- so heavily weighted towards the zeros 


# Table 1 Summary Statistics

```{r}
rf_acc <- 0.782
rf_auc <- 0.630
xg_mae <- 0.242
xg_auc <- 0.601
lasso_mae <- 0.249
lasso_auc <- 0.664
ensemble_auc <- 0.639
table_df <- data.frame(
  Model = c("Random Forest", "XGBoost", "Penalized Regression - Lasso", "Ensemble"),
  Accuracy = c(rf_acc, "", "", ""),
  MAE = c("", xg_mae, lasso_mae, ""),
  AUC = c(rf_auc, xg_auc, lasso_auc, ensemble_auc)
)

table_df %>% 
  kable(padding = -5L) %>% 
  kable_styling(
    bootstrap_options = "striped", 
    full_width = F, 
    fixed_thead = T
    #extra_css = "td { line-height: 0.5; }"
  )
```





